{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第八章 循环神经网络\n",
    "\n",
    "循环神经网络与我们前面接触过的前馈神经网络和卷积神经网络最大的不同是有一些“记忆暂存”功能，可以把过去输入的内容所产生的远期影响量化后与当前时间输入的内容一起反应到网络中去参与训练。这也就解决了很多原来的前馈神经网络对于上下文有关的场景处理有局限性的问题。\n",
    "\n",
    "现在业界使用循环神经网络还是很多的，尤其是在自然语言处理方面应用很广泛。"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8.1 隐马尔科夫模型\n",
    "\n",
    "马尔科夫链的核心是说，在给定当前知识或信息的情况下，观察对象过去的历史状态对于将来的预测来说是无关的。也就是说，在观察一个系统变化的时候，它下一个状态（第n+1个状态）如何的概率只需要观察和统计当前状态（第n个状态）即可得以正确得出。\n",
    "\n",
    "隐马尔可夫链是一个双重的随机过程，不仅状态转移之间是个随机事件，状态和输出之间也是一个随机过程。\n",
    "\n",
    "一般来说，HMM中说到的马尔科夫链其实是指隐含状态链，因为实际是银行状态之间存在转换概率。\n",
    "\n",
    "尽管可见状态之间没有直接的转换概率，但是隐含状态和可见状态之间有一个概率叫作输出概率。\n",
    "\n",
    "隐马尔科夫模型从给予样本序列到最后训练出来两个矩阵，应该是经历了一个非监督学习过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8.2 RNN和BPTT\n",
    "\n",
    "### 8.2.1 结构\n",
    "\n",
    "有两个待定系数，一个是$W_x$，一个是$W_H$，其中$W_x$会与$X_t$做向量乘积，作为输入的一部分。另一部分是由前一次输出的$H_{t-1}$和$W_H$相乘得到。等于说前一次计算输出的$H_{t-1}$需要缓存一下，在本次$X_t$输入的时候参与运算，共同输出最后的$Y$。$Y$也是一个向量，它是由前面输入的$H_{t-1}$和$W_H$相乘所产生的向量和$W_x$与$X_t$相乘所产生的向量加和后做SOFTMAX得到的。\n",
    "\n",
    "在这样一个模型里蕴含着这样的一个逻辑，那就是前一次输入的向量$X_{t-1}$所产生的结果对于本次输出的结果是由一定的影响的，甚至更远期的$X_{t-2}$、$X_{t-3}$……都“潜移默化”地影响本次输出的结果。这中间的具体量化的逻辑关系就是需要我们通过训练得到的，那就是得到待定的$W_x$和$W_H$矩阵。\n",
    "\n",
    "### 8.2.2 训练过程\n",
    "\n",
    "最简单的RNN模型在工作的时候是可以一个单元独立工作的。\n",
    "\n",
    "在训练的过程中，文字是没办法直接扔进去的，所以都会通过一个“Word to vector”的功能模块把字或词汇转换成为数字向量。这样，当$W_x$和$W_H$这两个矩阵被初始化之后，一定在$Y$一侧有输出的，那就一定有残差产生。设每个样本的残差的$E_i$，在一次完整的训练中，整个网络的残差就是从第一次扔进去对话的第一句和第二句的时候产生的$E_1$，加上第二句和第三句放进去的时候产生的$E_2$……一直加到导数第二句和导数第一句放进去的时候所产生的残差$E-{n-1}$，也就是可以简写成$$Loss=\\sum_{i=1}^{n-1}E_i$$\n",
    "\n",
    "### 8.2.3 艰难的误差传递\n",
    "\n",
    "由于循环的输入，导致了损失函数求导困难，所以传统的CNN虽然在理论上说的通，但是在训练中的效果是非常不理想的。在对RNN的改造中后人发现了一种叫做LSTM的算法替代BPTT算法来实现RNN的训练方式。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8.3 LSTM算法\n",
    "\n",
    "LSTM算法的全称是长短期记忆网络（long short-term memory），由LSTM算法对标准的RNN进行的改进，会得到LSTM网络。它规避了标准的RNN中的梯度爆炸和梯度消失的问题，所以会显得更好用一些，学习速度更快。\n",
    "\n",
    "现在在工业上，如果考虑使用RNN作为模型来训练的时候，通常也是直接使用LSTM网络。LSTM网络与传统的RNN网络相比多了一个非常有用的机制，就是**忘记门(forget gate)**。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8.4 应用场景\n",
    "\n",
    "RNN或是LSTM等循环神经网络，其结构比原先的BP网络和CNN网络都要复杂，尤其是它允许输入和输出都是多个值或者说多个向量，所以他的功能更为丰富。\n",
    "\n",
    "它可以做分类工具，可以做有限状态机（控制程序），可以做翻译，可以做聊天机器人等看上去很炫酷的事情。\n",
    "\n",
    "按照“可视化”归纳，大致可以分为“一到一”、“一到多”、“多到一”、“多到多”的映射种类。\n",
    "\n",
    "1. “一到一”，很简单；\n",
    "2. “一到多”，是一种单一向量输入，多向量输出的场景。具体来说，例如描述一张图上的信息，微软的实图机器人CaptionBot就是比较典型的例子；\n",
    "3."
   ]
  }
 ]
}