{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第十一章 强化学习\n",
    "\n",
    "强化学习（reinforcement learning），也有地方译作“增强学习”，严格说来不算深度学习的范畴。因为深度学习基本是在研究使用深度神经网络在处理各种问题时候的经验与技巧，而强化学习本身是一种人工智能在训练中得到策略的训练过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 11.1 模型核心\n",
    "\n",
    "另个重要因素，分别是“动作”或“行为”和“反馈”或“奖励”。\n",
    "\n",
    "1. 把奖励和损失定义好，让环境中产生的奖励和损失能够顺利有效地量化反馈给主体\n",
    "2. 让主体以较低的成本快速地不断尝试，以总结出在不同的State的情形下Reward较大的工作方式。\n",
    "\n",
    "用类似隐马尔科夫链的方式做训练，统计一下状态转换的概率和得到Reward的数学期望值，然后寻找一条获取最大Reward的路径。"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 11.2 马尔可夫决策过程\n",
    "\n",
    "马尔可夫决策过程（markov decision process，MDP），在一个状态下，会有多大的概率选择某一种动作，以及每一次状态的迁移会获得多大一个奖励。整个过程是从大量的样本学习中得到的，用表达式来表示这个模型通常写作：$(S，A，P(s，s'), R(s, s'))$这样一个四元组。\n",
    "\n",
    "- $S$表示状态State，\n",
    "- $A$表示动作Action，\n",
    "- $P(s，s')$表示前后两种状态$s$和$s'$之间的转化概率，\n",
    "- $R(s，s')$表示前后两种状态$s$和$s'$之间的转化所获得的奖励Reward。"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 11.3 深度学习中的Q-Learning——DQN"
   ]
  }
 ]
}