{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第7张 综合问题\n",
    "\n",
    "## 7.1 并行计算\n",
    "\n",
    "并行计算是用来加快深度神经网络训练的最直接的方式。\n",
    "\n",
    "## 7.2 随机梯度下降\n",
    "\n",
    "随机抽取一定数量的样本进行多轮（epoch）训练，这是一种用精确度换时间的一种代偿方案。\n",
    "\n",
    "## 7.3 梯度消失问题\n",
    "\n",
    "这种问题的发生会让训练很难进行下去，看到的现象就是训练不再收敛——$Loss$过早第不再下降，而精确度也过早地不再提高。\n",
    "\n",
    "初始化一个合适的w行不通，现在最为成熟的办法就是使用导数值比较合适的激励函数，比如ReLU函数。\n",
    "\n",
    "## 7.4 归一化\n",
    "\n",
    "机器学习中常见的归一化方法有线性函数归一化（min-max scaling），还有0均值标准化（z-score standardization）。目的都是为了让各维度的数据分布经过“拉伸”投射到一个相近的尺度范围去。\n",
    "\n",
    "线性函数归一化：\n",
    "\n",
    "$$X_{norm}={{X-X_{min}}\\over{X_{max}-X_{min}}}$$\n",
    "\n",
    "每一个数值经过这样一个投射就都变成0到1之间的数值了，这个值表示自己在整个样本中该维度所处的位置比例。\n",
    "\n",
    "在深度学习中同样会用到归一化的问题，常见的是用叫做Batch Normalization的归一化过程，也翻译成批归一化。在整个网络中的任何一层都可以加入批归一化过程，等于将每层网络都看成一个独立的分类模型，这样可以让网络尽可能避免没有数据分布的不同所带来的尴尬。\n",
    "\n",
    "在工程中加入Batch Normalization可以在一定程度上避免过拟合的发生，加强泛化能力。\n",
    "\n",
    "## 7.5 参数初始化问题\n",
    "\n",
    "一种相对来说业界比较认可的说法是把整个网络中所有的$w$初始化成以0位均值$\\mu$、以某个很小的值$\\sigma$为标准差的正态分布的方式通常效果会比较好。\n",
    "\n",
    "## 7.6 正则化\n",
    "\n",
    "正则化之一过程式帮助我们找到更为简洁的描述方式的量化过程。对损失函数的改造：\n",
    "\n",
    "$$C=C_0+{{\\lambda}\\over{n}}\\sum_w|w|$$\n",
    "\n",
    "$C_0$是原本的损失函数\n",
    "\n",
    "在公式中的后增加的部分的含义是把整个模型中所有的权重$w$的绝对值加起来除以样本数量。其中$\\lambda$是一个权重——也可以称为**正则化系数**或**惩罚系数**，表示对这个部分有多“重视”。这个正则化因子叫做L1正则化项，常用的还有一种叫带有L2的正则化项。\n",
    "\n",
    "$$C=C_0+{{\\lambda}\\over{2n}}\\sum_ww^2$$\n",
    "\n",
    "带有L1正则项的导数：\n",
    "$$C=C_0+{{\\lambda}\\over{n}}\\sum_w|w|$$\n",
    "$${{\\partial C}\\over{\\partial w}}={{\\partial C_0}\\over{\\partial w}}+{{\\lambda}\\over{n}}{\\rm sgn}(w)$$\n",
    "$$(w)^n=(w)^{n-1}-\\eta{{\\partial C_0}\\over{\\partial w}}-\\eta{{\\lambda}\\over{n}}{\\rm sgn}(w)$$\n",
    "其中${\\rm sgn}(w)$表示取$w$的符号，大于0就是正1，小于0就是-1。\n",
    "\n",
    "带有L2的正则项导数为：\n",
    "$$C=C_0+{{\\lambda}\\over{2n}}\\sum_ww^2$$\n",
    "$${{\\partial C}\\over{\\partial w}}={{\\partial C_0}\\over{\\partial w}}+{{\\lambda}\\over{n}}w$$\n",
    "$${{\\partial C}\\over{\\partial b}}={{\\partial C_0}\\over{\\partial b}}$$\n",
    "$$(w)^n=(w)^{n-1}-\\eta{{\\partial C_0}\\over{\\partial w}}-\\eta{{\\lambda}\\over{n}}w$$\n",
    "\n",
    "在一次模型搭建的过程中，通常先不加入正则化项，先只用带有经验风险项的损失函数来训练模型，当模型训练结束后，再尝试加入正则化项来进行改进。这个$\\lambda$可以设置为1、5、10、15、20……这样的方法往下试，也可以用类似1、100、50、25、75……这种二分法的方式去设置，去观察当前加入的这个$\\lambda$值是不是有效地提高了准确率$Accuracy$。这个试探的过程式没办法避免的。\n",
    "\n",
    "## 7.7 其他超参数\n",
    "\n",
    "超参数（hyper parameter）通常指的是那些在机器学习算法训练的步骤开始之前设定的一些参数值，这些参数值通常是没办法通过算法本身来学会的——与其相对的就是在算法中可以学会或学的到那些参数。\n",
    "\n",
    "例如K-Means算法中的簇数$N$，还有就是像在深度学习中涉及的学习率$\\eta$。这个$\\eta$通常来说应该给一个比较小的值。\n",
    "\n",
    "## 7.8 不唯一的模型\n",
    "\n",
    "即便训练样本都一样，第一次训练出来的模型和第二次训练出来的模型（即便是经历同样的轮数），得到了相近的损失函数$Loss$的值和准确率$Accuracy$的值，也很可能得到不完全一样的模型结构。\n",
    "\n",
    "首先是在随机梯度下降算法中使用的Mini Batch，等于从众多的数据中选择了一部分数据用来训练模型，那么从$Loss$较高的位置项哪个方向移动可就跟每一批次被选入的样本特征有关了。\n",
    "\n",
    "除此之外，权值在网络初始化的时候也是随机性地初始化的。"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7.9 DropOut\n",
    "\n",
    "DropOut是在深度学习训练中较为常用的方法，主要也是为了用于克服过拟合现象。\n",
    "\n",
    "DropOut——顾名思义是“丢弃”，丢弃什么呢？在一轮训练阶丢弃一部分网络节点。比如可以在其中的某些层上临时关闭一些节点，让它们不输入也不输出，这样相当于整个网络的结构发生了变化。而在下一轮训练的过程中再选择性地临时关闭一些节点，原则上都是随机性的。\n",
    "\n",
    "```flow\n",
    "st=>start:Start\n",
    "io=>inputoutput:verfication\n",
    "e=>end\n",
    "\n",
    "st->io->end\n",
    "```"
   ]
  }
 ]
}