{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第十章 受限玻尔兹曼机"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10.1 结构\n",
    "\n",
    "在训练时有如下关系：\n",
    "\n",
    "$$p(h_i=1|v)=\\sigma(\\sum_{j=1}^mw_{ij}\\times v_j+c_i)， p(v_j=1|h)=\\sigma(\\sum_{i=1}^nw_{ij}\\times h_i+b_i)$$\n",
    "\n",
    "这种模型有趣的地方就在于输入输出两侧同时放入样本，可以互为输入和输出形成一个映射关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10.2 逻辑回归\n",
    "\n",
    "Sigmoid函数的推导。\n",
    "\n",
    "伯努利分布其实就二项分布。\n",
    "\n",
    "$$\n",
    "P_n=\\begin{cases}\n",
    "\\quad p,\\quad n=1\\\\\n",
    "1-p, \\quad n=0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "它们的比值s为：\n",
    "$$p\\over1-p$$\n",
    "\n",
    "取$t=ln(s)$，则有如下推导：\n",
    "$$t=ln({{}p\\over{1-p}})\\$$\n",
    "$$\\Longleftrightarrow e^t={{p}\\over{1-p}}$$\n",
    "$$\\Longleftrightarrow e^t(1-p)=p$$\n",
    "$$\\Longleftrightarrow e^t=p+e^tp$$\n",
    "$$\\Longleftrightarrow p={{1}\\over{1+e^{-t}}}$$\n",
    "\n",
    "将t替换成$wx+b$，$f(x)$就是1产生的概率$p$，$x$就是一个多维向量。也就是：$$p=f(x)={{1}\\over{1+e^{-(wx+b)}}}$$\n",
    "\n",
    "逻辑回归的损失函数：\n",
    "$$Loss=-{{1}\\over{n}}\\sum_{i=1}^ny_i\\cdot{\\rm log}f(x_i)+(1-y_i)\\cdot{\\rm log}\\big{(}1-f(x_i)\\big{)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10.3 最大似然度\n",
    "\n",
    "逻辑回归的理论依据就是统计学中的最大似然度或最大似然估计。用白话来说，就是根据统计学特征估算其最大可能的情况这样一种方式。\n",
    "\n",
    "如果你观测到的$x$只有“1”和“0”两种情况，那么$P(x)$的表达式就是伯努利分布。\n",
    "\n",
    "如果你观测到的$x$是一个以$\\mu$为平均值，以$\\sigma$为标准差的正态分布，那么$P(x)$就应该表示为：\n",
    "$$P(x)={{1}\\over{\\sqrt{2\\pi}}}e^{{(x-\\mu)^2}\\over{2\\sigma^2}}$$\n",
    "\n",
    "观测值$x_1$，$x_2$，……，$x_r$的联合密度函数应为：$$L(x|\\theta)=\\prod_{t=1}^rP(x_i)$$\n",
    "这个$L$函数的含义就是似然度。\n",
    "\n",
    "对于一个确定项的$L(x|\\theta)$函数，希望求出一个合适的$\\theta$向量的数值使得这个连乘式取得最大值，就是在边倒数等于0的点作为候选去找,即\n",
    "$${{\\partial L(x|\\theta)}\\over{\\partial \\theta}}=0$$\n",
    "\n",
    "对于连乘的优化处理是取其对数，使得乘法变成了加法。\n",
    "\n",
    "在线性回归中，从学术的角度来说，对于一个完成的$$X=w_1x_1+w_2x_2+\\dots+w_nx_n+b+u$$来说，$x_1$到$x_n$叫做“解释变量”，$u$叫作“随机扰动项”，是在随机过程中的一种不确定值，且这个不确定值一般是满足正态分布的：$$u\\sim N(0，\\sigma^2)$$ 即中心为0、方差为$\\sigma^2$的正态分布。\n",
    "\n",
    "$X$的分布为：$$X\\sim N(\\mu，\\sigma^2)，\\mu=w_1x_1+w_2x_2+\\dots+w_nx_n+b$$\n",
    "$\\mu$这个等式就是$wx+b$的由来。\n",
    "\n",
    "继续推导：\n",
    "$$\n",
    "\\begin{align}\n",
    "L(w,\\sigma^2) &=P(x_1，x_2，\\dots，x_r)\\\\\n",
    "              &=\\prod_{t=1}^TP(x_t)\\\\\n",
    "              &={{1}\\over{(2\\pi^{{r}\\over{2}})}\\sigma^T}e^{{1}\\over{2\\sigma^2}\\sum_{t=1}^r(x_t-\\mu_t)^2}\n",
    "\\end{align}\n",
    "$$\n",
    "求对数：\n",
    "$${\\rm ln}L(w，\\sigma^2)=\\sum_{t=1}^T\\Bigg{(}-{{1}\\over{2}}{\\rm ln}(2\\pi\\sigma^2)-{{1}\\over{2\\sigma^2}}(x_t-\\mu_t)^2\\Bigg{)}，t=1，2，\\dots，T$$\n",
    "这里的t就代表每个观测样本的序列号。\n",
    "\n",
    "**一个连乘关系的最大值可以通过求对数极值的方法找到最大值点，而且它最大值的位置和其取对数后的极大值位置是相同的。**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10.4 最大似然度示例\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 10.5 损失函数\n",
    "\n",
    "受限玻尔兹曼机的损失函数叫做对比散度函数(contrasive divergence)。"
   ]
  }
 ]
}