{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第六章 卷积神经网络\n",
    "\n",
    "神经网络不是一个具体的算法，二是一种模型构建的思路或者方式。\n",
    "\n",
    "## 6.1 与全连接网络的对比\n",
    "\n",
    "神经元之间彼此连接的方式有一个特点，那就是每一个神经元节点的输入都来自于上一层的每一个神经元的输出。这种方式就叫做**全连接网络（full connect network）————整个网络的每一层都以这种“全连接”方式完成的。全连接网络有一个很明显的缺点，训练过程中要更新的权重非常多，整个网络训练的收敛速度会非常慢。对于图像识别这种输入像素动辄数百万维度（以像素点为单位）的分类处理，就会变得不可行。这就要用到卷积神经网络（convolutional neural network，CNN）\n",
    "\n",
    "卷积神经网络同样是一种前馈神经网络，它的神经元可以响应一部分覆盖范围内的周围单元，对于大规模的模式识别都有着非常好的性能表现的，尤其是对大规模图形图像处理效率极高。\n",
    "\n",
    "卷积神经网络有两个比较大的特点：\n",
    "\n",
    "1. 卷积网络有至少一个卷积层，用来提取特征；\n",
    "2. 卷积网络的卷积层通过权值共享的方式进行工作，大大减少权值$w$的数量，使得在训练中在达到同样识别率的情况下收敛速度明显快于全连接BP网络。\n",
    "\n",
    "## 6.2 卷积是什么\n",
    "\n",
    "在泛函分析中，卷积（convolution）是一种函数的定义。**它是通过两个函数$f$和$g$生成第三个函数的一种数学算子，表征函数$f$与$g$经过翻转和平移的重叠部分的面积。**\n",
    "\n",
    "卷积的数学定义是这样的：\n",
    "\n",
    "$$h(x)=f(x)*g(x)=\\int_{-\\infty}^{+\\infty}f(t)g(x-t)dt\\,$$\n",
    "\n",
    "## 6.3 卷积核\n",
    "\n",
    "卷积核的表达式：$f(x)=wx+b$\n",
    "\n",
    "卷积核的$w$合$b$的值，通过一轮一轮的训练，降低损失函数而得。\n",
    "\n",
    "在卷积核输出后，还可能会跟着一个激励函数，而且一般也会定义一个激励函数跟随其后。现在的CNN网络中的激励函数非常喜欢用ReLU。\n",
    "\n",
    "## 6.4 卷积层的其他参数\n",
    "\n",
    "在卷积核对面前输入的这一层数据向量进行扫描的时候，还有几个别的参数需要注意，一个是Padding(填充)，一个是Striding(步幅)。\n",
    "\n",
    "Padding是指用多少个像素单位来填充输入图像（向量）的边界。通常都是填充0值。\n",
    "\n",
    "Padding的用户大概可以理解为两种目的：\n",
    "1. 保持边界信息。\n",
    "2. 如果输入的图片尺寸有差异，可以通过Padding来进行补齐，使得输入的尺寸一致，以免频繁调整卷积核和其他层的工作模式。\n",
    "\n",
    "Stride就是步幅，在卷积层工作的时候，Stride可以理解为每次滑动的单位。\n",
    "\n",
    "## 6.5 池化层\n",
    "\n",
    "池化层（Pooling Layer，或称池层）是在一些旧有的CNN网络中喜欢设计的一层处理层。池化层的作用实际上对Feature Map所做的数据处理又进行了一次所谓的池化处理。\n",
    "\n",
    "常见的池化处理有两种方式：一种叫Max Pooling，一种叫Mean Pooling(也叫Average Pooling)，顾名思义，一个做了最大化，一个做了平均化。除此之外还有Chunk-Max Pooling、Stochastic Pooling等其他一些池化手段。\n",
    "\n",
    "一般来说，池化层被认为有这样几个功能\n",
    "1. 它又进行了一次特征提取，所以肯定是能够减小下一层数据的处理量\n",
    "2. 由于这个特征的提取，能够有更大的可能性进一步获取更为抽象的信息，从而防止过拟合，或者说提高一定的泛化性。\n",
    "3. 由于这种抽象性，所以能够对输入的微小变化产生更大的容忍，也就是保持其不变性。这里的容忍包括图形的少量平移、旋转以及缩放等变化。\n",
    "\n",
    "池化层在CNN网络中不是一个必须的组件，一些新的CNN网络在设计的时候也没有池化层出现。\n",
    "\n",
    "## 6.6 典型神经网络"
   ]
  }
 ]
}